{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "y_train, x_train, id_train = load_csv_data('data/train.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "y_test, x_test, id_test = load_csv_data('data/test.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.33968680947703495\n",
      "Test loss: 0.9727587947107508\n"
     ]
    }
   ],
   "source": [
    "w, loss = least_squares(y_train, x_train)\n",
    "print(\"Train loss: \", loss)\n",
    "print(\"Test loss:\", compute_loss(y_test, x_test, w))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9300128601875837\n"
     ]
    }
   ],
   "source": [
    "lambdas = np.arange(0.01, 1, 0.01)\n",
    "losses = []\n",
    "for i in range(len(lambdas)):\n",
    "    w, loss = ridge_regression(y_train,x_train, i)\n",
    "    losses.append(compute_loss(y_test, x_test, w))\n",
    "print(np.min(losses))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vidmy\\PycharmProjects\\ML_project\\ML_Higgs\\implementations.py:166: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-t))\n",
      "C:\\Users\\vidmy\\PycharmProjects\\ML_project\\ML_Higgs\\implementations.py:38: RuntimeWarning: divide by zero encountered in log\n",
      "  y.T.dot(np.log(sigmoid(tx.dot(w))))\n",
      "C:\\Users\\vidmy\\PycharmProjects\\ML_project\\ML_Higgs\\implementations.py:39: RuntimeWarning: divide by zero encountered in log\n",
      "  + (1 - y).T.dot(np.log(1 - sigmoid(tx.dot(w))))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [19]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m gammas \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marange(\u001B[38;5;241m0.1\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0.1\u001B[39m)\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(lambdas)):\n\u001B[1;32m----> 3\u001B[0m     w, loss \u001B[38;5;241m=\u001B[39m \u001B[43mlogistic_regression\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mones\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m30\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m     i\n\u001B[0;32m      5\u001B[0m     losses\u001B[38;5;241m.\u001B[39mappend(compute_loss(y_test, x_test, w))\n",
      "File \u001B[1;32m~\\PycharmProjects\\ML_project\\ML_Higgs\\implementations.py:205\u001B[0m, in \u001B[0;36mlogistic_regression\u001B[1;34m(y, tx, initial_w, max_iters, gamma)\u001B[0m\n\u001B[0;32m    203\u001B[0m loss \u001B[38;5;241m=\u001B[39m calculate_loss_logistic(y, tx, w)\n\u001B[0;32m    204\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m n_iter \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(max_iters):\n\u001B[1;32m--> 205\u001B[0m     loss, w \u001B[38;5;241m=\u001B[39m \u001B[43mlearning_by_gradient_descent\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mw\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgamma\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    206\u001B[0m loss \u001B[38;5;241m=\u001B[39m calculate_loss_logistic(y, tx, w)\n\u001B[0;32m    207\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m w, loss\n",
      "File \u001B[1;32m~\\PycharmProjects\\ML_project\\ML_Higgs\\implementations.py:184\u001B[0m, in \u001B[0;36mlearning_by_gradient_descent\u001B[1;34m(y, tx, w, gamma)\u001B[0m\n\u001B[0;32m    169\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlearning_by_gradient_descent\u001B[39m(y, tx, w, gamma):\n\u001B[0;32m    170\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    171\u001B[0m \u001B[38;5;124;03m    Do one step of gradient descent using logistic regression. Return the loss and the updated w.\u001B[39;00m\n\u001B[0;32m    172\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    182\u001B[0m \n\u001B[0;32m    183\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 184\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[43mcalculate_loss_logistic\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    185\u001B[0m     w \u001B[38;5;241m=\u001B[39m w \u001B[38;5;241m-\u001B[39m gamma \u001B[38;5;241m*\u001B[39m (calculate_gradient_logistic(y, tx, w))\n\u001B[0;32m    186\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss, w\n",
      "File \u001B[1;32m~\\PycharmProjects\\ML_project\\ML_Higgs\\implementations.py:38\u001B[0m, in \u001B[0;36mcalculate_loss_logistic\u001B[1;34m(y, tx, w)\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;124;03m\"\"\"compute the cost by negative log likelihood.\u001B[39;00m\n\u001B[0;32m     22\u001B[0m \n\u001B[0;32m     23\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;124;03m    a non-negative loss\u001B[39;00m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;66;03m# ***************************************************\u001B[39;00m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;66;03m# INSERT YOUR CODE HERE\u001B[39;00m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;66;03m# ***************************************************\u001B[39;00m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39msum(\n\u001B[0;32m     36\u001B[0m     \u001B[38;5;241m-\u001B[39m(\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m/\u001B[39m y\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m     37\u001B[0m     \u001B[38;5;241m*\u001B[39m (\n\u001B[1;32m---> 38\u001B[0m         y\u001B[38;5;241m.\u001B[39mT\u001B[38;5;241m.\u001B[39mdot(np\u001B[38;5;241m.\u001B[39mlog(\u001B[43msigmoid\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mw\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m))\n\u001B[0;32m     39\u001B[0m         \u001B[38;5;241m+\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m y)\u001B[38;5;241m.\u001B[39mT\u001B[38;5;241m.\u001B[39mdot(np\u001B[38;5;241m.\u001B[39mlog(\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m sigmoid(tx\u001B[38;5;241m.\u001B[39mdot(w))))\n\u001B[0;32m     40\u001B[0m     )\n\u001B[0;32m     41\u001B[0m )\n",
      "File \u001B[1;32m~\\PycharmProjects\\ML_project\\ML_Higgs\\implementations.py:166\u001B[0m, in \u001B[0;36msigmoid\u001B[1;34m(t)\u001B[0m\n\u001B[0;32m    156\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msigmoid\u001B[39m(t):\n\u001B[0;32m    157\u001B[0m     \u001B[38;5;124;03m\"\"\"apply sigmoid function on t.\u001B[39;00m\n\u001B[0;32m    158\u001B[0m \n\u001B[0;32m    159\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    164\u001B[0m \n\u001B[0;32m    165\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 166\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexp\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43mt\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "gammas = np.arange(0.1, 1, 0.1)\n",
    "for i in range(len(lambdas)):\n",
    "    w, loss = logistic_regression(y_train, x_train, np.ones(30), 10, i)\n",
    "    losses.append(compute_loss(y_test, x_test, w))\n",
    "print(np.min(losses))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ada')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d00fa60d55de8b7e410201337c62de4ddc89fbea421e7cf3f923f4aa4ba8995"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
